{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing sparse user adjacency matrices\n",
    "\n",
    "This notebook constructs sparse user matrices for downstream clustering / other analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import itertools as itt\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import json\n",
    "import ctypes as ct\n",
    "\n",
    "path = \"../../data/users/\"\n",
    "machine = 'vm2'\n",
    "min_activity = 100\n",
    "interaction_type = 'directs' #can also be 'indirects'\n",
    "\n",
    "assert interaction_type in ['directs', 'indirects'], \"Invalid type of interaction data provided\"\n",
    "data_col = 4 if interaction_type == 'directs' else 5\n",
    "\n",
    "files = [f.absolute() for f in Path(path).glob(\"*.csv\")]\n",
    "user_stats = path + 'summaries/combined/user_stats.csv'\n",
    "user_interaction_map = f\"{path}summaries/{machine}/interaction_map-{interaction_type}-min-{min_activity}.csv\"\n",
    "adj_matrix_path = path + f\"{path}summaries/{machine}/adj_matrix-{interaction_type}-min-{min_activity}.npz\"\n",
    "csv.field_size_limit(int(ct.c_ulong(-1).value // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users: 7810\n"
     ]
    }
   ],
   "source": [
    "#first, we construct a dictionary to map unique user names to indices\n",
    "iterator = itt.count()\n",
    "user_names = pl.read_csv(user_stats).filter(pl.col(\"total_activity\") >= min_activity).select(\"user_name\").to_dict(as_series=False)[\"user_name\"]\n",
    "mapped = {u: next(iterator) for u in user_names}\n",
    "del user_names\n",
    "print(\"Total number of users: {}\".format(len(mapped)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1 out of 127\n",
      "Processing file 2 out of 127\n",
      "Processing file 3 out of 127\n",
      "Processing file 4 out of 127\n",
      "Processing file 5 out of 127\n",
      "Processing file 6 out of 127\n",
      "Processing file 7 out of 127\n",
      "Processing file 8 out of 127\n",
      "Processing file 9 out of 127\n",
      "Processing file 10 out of 127\n",
      "Processing file 11 out of 127\n",
      "Processing file 12 out of 127\n",
      "Processing file 13 out of 127\n",
      "Processing file 14 out of 127\n",
      "Processing file 15 out of 127\n",
      "Processing file 16 out of 127\n",
      "Processing file 17 out of 127\n",
      "Processing file 18 out of 127\n",
      "Processing file 19 out of 127\n",
      "Processing file 20 out of 127\n",
      "Processing file 21 out of 127\n",
      "Processing file 22 out of 127\n",
      "Processing file 23 out of 127\n",
      "Processing file 24 out of 127\n",
      "Processing file 25 out of 127\n",
      "Processing file 26 out of 127\n",
      "Processing file 27 out of 127\n",
      "Processing file 28 out of 127\n",
      "Processing file 29 out of 127\n",
      "Processing file 30 out of 127\n",
      "Processing file 31 out of 127\n",
      "Processing file 32 out of 127\n",
      "Processing file 33 out of 127\n",
      "Processing file 34 out of 127\n",
      "Processing file 35 out of 127\n",
      "Processing file 36 out of 127\n",
      "Processing file 37 out of 127\n",
      "Processing file 38 out of 127\n",
      "Processing file 39 out of 127\n",
      "Processing file 40 out of 127\n",
      "Processing file 41 out of 127\n",
      "Processing file 42 out of 127\n",
      "Processing file 43 out of 127\n",
      "Processing file 44 out of 127\n",
      "Processing file 45 out of 127\n",
      "Processing file 46 out of 127\n",
      "Processing file 47 out of 127\n",
      "Processing file 48 out of 127\n",
      "Processing file 49 out of 127\n",
      "Processing file 50 out of 127\n",
      "Processing file 51 out of 127\n",
      "Processing file 52 out of 127\n",
      "Processing file 53 out of 127\n",
      "Processing file 54 out of 127\n",
      "Processing file 55 out of 127\n",
      "Processing file 56 out of 127\n",
      "Processing file 57 out of 127\n",
      "Processing file 58 out of 127\n",
      "Processing file 59 out of 127\n",
      "Processing file 60 out of 127\n",
      "Processing file 61 out of 127\n",
      "Processing file 62 out of 127\n",
      "Processing file 63 out of 127\n",
      "Processing file 64 out of 127\n",
      "Processing file 65 out of 127\n",
      "Processing file 66 out of 127\n",
      "Processing file 67 out of 127\n",
      "Processing file 68 out of 127\n",
      "Processing file 69 out of 127\n",
      "Processing file 70 out of 127\n",
      "Processing file 71 out of 127\n",
      "Processing file 72 out of 127\n",
      "Processing file 73 out of 127\n",
      "Processing file 74 out of 127\n",
      "Processing file 75 out of 127\n",
      "Processing file 76 out of 127\n",
      "Processing file 77 out of 127\n",
      "Processing file 78 out of 127\n",
      "Processing file 79 out of 127\n",
      "Processing file 80 out of 127\n",
      "Processing file 81 out of 127\n",
      "Processing file 82 out of 127\n",
      "Processing file 83 out of 127\n",
      "Processing file 84 out of 127\n",
      "Processing file 85 out of 127\n",
      "Processing file 86 out of 127\n",
      "Processing file 87 out of 127\n",
      "Processing file 88 out of 127\n",
      "Processing file 89 out of 127\n",
      "Processing file 90 out of 127\n",
      "Processing file 91 out of 127\n",
      "Processing file 92 out of 127\n",
      "Processing file 93 out of 127\n",
      "Processing file 94 out of 127\n",
      "Processing file 95 out of 127\n",
      "Processing file 96 out of 127\n",
      "Processing file 97 out of 127\n",
      "Processing file 98 out of 127\n",
      "Processing file 99 out of 127\n",
      "Processing file 100 out of 127\n",
      "Processing file 101 out of 127\n",
      "Processing file 102 out of 127\n",
      "Processing file 103 out of 127\n",
      "Processing file 104 out of 127\n",
      "Processing file 105 out of 127\n",
      "Processing file 106 out of 127\n",
      "Processing file 107 out of 127\n",
      "Processing file 108 out of 127\n",
      "Processing file 109 out of 127\n",
      "Processing file 110 out of 127\n",
      "Processing file 111 out of 127\n",
      "Processing file 112 out of 127\n",
      "Processing file 113 out of 127\n",
      "Processing file 114 out of 127\n",
      "Processing file 115 out of 127\n",
      "Processing file 116 out of 127\n",
      "Processing file 117 out of 127\n",
      "Processing file 118 out of 127\n",
      "Processing file 119 out of 127\n",
      "Processing file 120 out of 127\n",
      "Processing file 121 out of 127\n",
      "Processing file 122 out of 127\n",
      "Processing file 123 out of 127\n",
      "Processing file 124 out of 127\n",
      "Processing file 125 out of 127\n",
      "Processing file 126 out of 127\n"
     ]
    }
   ],
   "source": [
    "def process_batch_file(filename, writer = None, data_column_no = 4, user_column_no = 8):\n",
    "    with open(filename) as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        next(reader) #skip the header\n",
    "        for i, row in enumerate(reader):\n",
    "            interactions = json.loads(row[data_column_no])\n",
    "            user_name = row[user_column_no]\n",
    "            for interlocutor, intensity in interactions.items():                \n",
    "                #get IDs\n",
    "                try:\n",
    "                    userid = mapped[user_name]\n",
    "                    interid = mapped[interlocutor]                    \n",
    "                    writer.writerow([userid, interid, intensity])\n",
    "                except KeyError:\n",
    "                    # ignore cases where there is no user in the mapped list\n",
    "                    # this only happens because mapped list is pre-filtered\n",
    "                    pass\n",
    "\n",
    "\n",
    "with open(user_interaction_map, \"w\") as outfile:\n",
    "    csvwriter = csv.writer(outfile, delimiter=',', quoting=csv.QUOTE_NONE)\n",
    "    for i,f in enumerate(files):\n",
    "        print(\"Processing file {} out of {}\".format(i + 1, len(files)))        \n",
    "        process_batch_file(f, csvwriter, data_column_no=data_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a sparse matrix\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "data = np.loadtxt(user_interaction_map, np.int64, delimiter=',')\n",
    "adj_matrix = sps.coo_matrix((data[:,2], (data[:, 0], data[:, 1])))\n",
    "sps.save_npz(adj_matrix_path, adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!f='interaction_map-directs-min-100.csv'\n",
    "!awk '(NR == 1) || (FNR > 1)' aurimas.eu/$f local/$f vm1/$f vm2/$f > combined/$f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining interaction maps from the 4 sources into a single map\n",
    "Fastest to combine interaction map CSVs using command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../../data/users/summaries/combined/\"\n",
    "int_map_path = path + 'interaction_map-directs-min-100.csv'\n",
    "adj_matrix_path = path + 'adjacency_matrix.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(int_map_path, np.int64, delimiter=',')\n",
    "adj_matrix = coo_matrix((data[:,2], (data[:, 0], data[:, 1])))\n",
    "save_npz(adj_matrix_path, adj_matrix)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "458e57744219fd35e36fe617bee87769eccb7cd58eff0111259e94f378fae128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
